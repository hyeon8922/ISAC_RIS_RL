import os
import numpy as np
import torch
import matplotlib.pyplot as plt

from channel import (
    load_and_process_data,
    generate_w,
    cal_loss,
    linear_to_db
)

#from ddpg import DDPGAgent as Agent
from TD3 import TD3Agent as Agent

# ============================================================
# ÌôòÍ≤Ω ÏÑ§Ï†ï
# ============================================================
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
device = 'cpu'
torch.set_default_dtype(torch.float32)

K, M, N = 1, 8, 32
lr = 1e-3
sigma2 = 1e-2
TaudB = 10
Tau = 10 ** (TaudB / 10)
batch_size = 200
Episode = 30
gamma = 0.99
tau_soft = 0.01
PdB = 10
P = 10 ** (PdB / 10)

alpha_corr = 1.0
beta_w = 0.8
lambda_ = 0.5

SNR_com_th_dB = TaudB
eps_corr = 1e-8

# ==========================================
# Train/Test Îç∞Ïù¥ÌÑ∞
# ==========================================
Train_Files = [
    f"C:/Users/CNL-A2/Desktop/DL-Beamforming-RIS-ISAC-main/channel/data/Train_Channel_{i:03d}.mat"
    for i in range(1, 30)
]

Test_File = "C:/Users/CNL-A2/Desktop/DL-Beamforming-RIS-ISAC-main/channel/data/Test_Channel.mat"


# ============================================================
# Îç∞Ïù¥ÌÑ∞ Î∂àÎü¨Ïò§Í∏∞
# ============================================================
train_sets = []
for f in Train_Files:
    R_sep_scaled, origin, originH, num = load_and_process_data(f, N, M)
    train_sets.append((R_sep_scaled.to(device), origin.to(device), originH.to(device), num))

R_test_sep_scaled, origin_dataset_test, origin_datasetH_test, test_num = \
    load_and_process_data(Test_File, N, M, is_test=True)

R_test_sep_scaled = R_test_sep_scaled.to(device).float()
origin_dataset_test = origin_dataset_test.to(device)
origin_datasetH_test = origin_datasetH_test.to(device)

# ============================================================
# üí° STATE DIM ÏàòÏ†ï: prev_SNRt + prev_SNRc ‚Üí 2Í∞ú Ï∂îÍ∞Ä
# ============================================================
state_dim = 4 * N * N + 2


# ============================================================
# ÌïôÏäµ
# ============================================================
if __name__ == "__main__":

    agent = Agent(
        N=N, device=device, lr=lr, gamma=gamma, tau=tau_soft,
        use_ou_noise=True, ou_rho=0.9, ou_sigma=0.2
    )

    episode_reward_log = []
    episode_snrt_log = []
    episode_snrc_log = []
    train_losses = []

    for e in range(Episode):

        agent.reset_noise()
        agent.actor.train()

        total_reward = 0.0
        step_count = 0

        SNRt_ep_list = []
        SNRc_ep_list = []

        # Ïó¨Îü¨ Trajectory Î∞òÎ≥µ
        for (R_sep_scaled, origin_dataset, origin_datasetH, total_num) in train_sets:

            # Ï¥àÍ∏∞ prev SNR
            prev_SNRt = 0.0
            prev_SNRc = 0.0

            for t in range(5, total_num):

                # --------------------------------------------------
                # 1) STATE Íµ¨ÏÑ±
                # --------------------------------------------------
                R_t5 = R_sep_scaled[t - 5]

                state_vec = torch.cat([
                    R_t5.reshape(-1),
                    torch.tensor([prev_SNRt, prev_SNRc],
                                 dtype=torch.float32, device=device)
                ], dim=0)

                state_batch = state_vec.unsqueeze(0)


                # --------------------------------------------------
                # 2) ACTION ÏÑ†ÌÉù
                # --------------------------------------------------
                theta_pred, thetaH_pred = agent.select_action(state_batch, noise=True)


                # --------------------------------------------------
                # 3) Channel Í≥ÑÏÇ∞
                # --------------------------------------------------
                y_chan = origin_dataset[t].unsqueeze(0)
                yH_chan = origin_datasetH[t].unsqueeze(0)

                _, hc_batch, ht_batch, Ht_batch = cal_loss(
                    y_chan, yH_chan, theta_pred, thetaH_pred, M, N
                )

                ht_i = ht_batch[0]
                hc_i = hc_batch[0]
                Ht_i = Ht_batch[0]


                # --------------------------------------------------
                # 4) w ÏÉùÏÑ±
                # --------------------------------------------------
                w = generate_w(ht_i, hc_i, P, Tau, sigma2)


                # --------------------------------------------------
                # 5) SNR Í≥ÑÏÇ∞
                # --------------------------------------------------
                SNRt_lin = torch.linalg.norm(Ht_i @ w)**2 / sigma2
                SNRc_lin = torch.abs(w.conj().T @ hc_i)**2 / sigma2

                SNRt_dB = linear_to_db(SNRt_lin)
                SNRc_dB = linear_to_db(SNRc_lin)

                SNRt_ep_list.append(SNRt_dB.item())
                SNRc_ep_list.append(SNRc_dB.item())


                # --------------------------------------------------
                # 6) reward Í≥ÑÏÇ∞
                # --------------------------------------------------
                norm_ht2 = torch.linalg.norm(ht_i)**2
                norm_hc2 = torch.linalg.norm(hc_i)**2
                corr_num = torch.abs(torch.conj(ht_i).T @ hc_i)**2
                rho_t = corr_num / (norm_ht2 * norm_hc2 + eps_corr)

                penalty = torch.nn.functional.relu(SNR_com_th_dB - SNRc_dB)

                reward_t = (
                    alpha_corr * rho_t +
                    beta_w * SNRt_dB +
                    (1.0 - beta_w) * SNRc_dB
                )


                # --------------------------------------------------
                # 7) NEXT-STATE Íµ¨ÏÑ±
                #    ‚≠ê ÏöîÏ≤≠ÏÇ¨Ìï≠ Î∞òÏòÅ: curr_SNRt, curr_SNRc Ìè¨Ìï® ‚≠ê
                # --------------------------------------------------
                R_next = R_sep_scaled[t - 4]

                next_state_vec = torch.cat([
                    R_next.reshape(-1),
                    torch.tensor([SNRt_dB.item(), SNRc_dB.item()],
                                 dtype=torch.float32, device=device)
                ], dim=0)


                # --------------------------------------------------
                # 8) Replay Ï†ÄÏû•
                # --------------------------------------------------
                agent.replay_buffer.add((
                    state_vec.detach().cpu(),
                    theta_pred[0].detach().cpu(),
                    float(reward_t.item()),
                    next_state_vec.detach().cpu(),
                    0.0
                ))


                # --------------------------------------------------
                # 9) ÌïôÏäµ ÏóÖÎç∞Ïù¥Ìä∏
                # --------------------------------------------------
                agent.train(batch_size)

                total_reward += reward_t.item()
                step_count += 1

                # Îã§Ïùå stateÏóêÏÑú ÏÇ¨Ïö©Ìï† prev_SNR ÏóÖÎç∞Ïù¥Ìä∏
                prev_SNRt = float(SNRt_dB.item())
                prev_SNRc = float(SNRc_dB.item())

        avg_reward = total_reward / max(step_count, 1)
        avg_snrt = np.mean(SNRt_ep_list)
        avg_snrc = np.mean(SNRc_ep_list)

        episode_reward_log.append(avg_reward)
        episode_snrt_log.append(avg_snrt)
        episode_snrc_log.append(avg_snrc)
        train_losses.append(-avg_reward)

        print(f"Ep {e+1:03d} | Reward={avg_reward:.4f} | "
              f"SNRt={avg_snrt:.2f} dB | SNRc={avg_snrc:.2f} dB")


    # ============================================================
    # ‚≠ê Test evaluation ‚≠ê
    # ============================================================
    print("\n=== Test evaluation ÏãúÏûë ===")
    agent.actor.eval()

    SNRt_test_list = []
    SNRc_test_list = []
    reward_test_list = []

    prev_SNRt_eval = 0.0
    prev_SNRc_eval = 0.0

    for t in range(5, test_num):

        R_t5 = R_test_sep_scaled[t - 5]

        # state Íµ¨ÏÑ±
        state_vec = torch.cat([
            R_t5.reshape(-1),
            torch.tensor([prev_SNRt_eval, prev_SNRc_eval],
                         dtype=torch.float32, device=device)
        ], dim=0)

        state_batch = state_vec.unsqueeze(0)

        # ÌñâÎèô
        theta_eval, thetaH_eval = agent.select_action(state_batch, noise=False)

        # channel
        y_chan = origin_dataset_test[t].unsqueeze(0)
        yH_chan = origin_datasetH_test[t].unsqueeze(0)

        _, hc_batch, ht_batch, Ht_batch = cal_loss(
            y_chan, yH_chan, theta_eval, thetaH_eval, M, N
        )

        ht_i = ht_batch[0]
        hc_i = hc_batch[0]
        Ht_i = Ht_batch[0]

        # w ÏÉùÏÑ±
        w_te = generate_w(ht_i, hc_i, P, Tau, sigma2)

        SNRt_lin = torch.linalg.norm(Ht_i @ w_te)**2 / sigma2
        SNRc_lin = torch.abs(w_te.conj().T @ hc_i)**2 / sigma2

        SNRt_test_list.append(SNRt_lin)
        SNRc_test_list.append(SNRc_lin)

        SNRt_dB = linear_to_db(SNRt_lin)
        SNRc_dB = linear_to_db(SNRc_lin)

        # reward
        norm_ht2 = torch.linalg.norm(ht_i)**2
        norm_hc2 = torch.linalg.norm(hc_i)**2
        corr_num = torch.abs(torch.conj(ht_i).T @ hc_i)**2
        rho_t = corr_num / (norm_ht2 * norm_hc2 + eps_corr)

        penalty = torch.nn.functional.relu(SNR_com_th_dB - SNRc_dB)

        reward_eval = alpha_corr * rho_t + beta_w * SNRt_dB + (1 - beta_w) * SNRc_dB
        reward_test_list.append(reward_eval.item())

        # next prev update
        prev_SNRt_eval = float(SNRt_dB.item())
        prev_SNRc_eval = float(SNRc_dB.item())

    SNRt_test = torch.stack(SNRt_test_list)
    SNRc_test = torch.stack(SNRc_test_list)
    reward_test_mean = np.mean(reward_test_list)

    print("\n=== Test Í≤∞Í≥º ===")
    print(f"SNRt_test = {linear_to_db(SNRt_test.mean()):.2f} dB")
    print(f"SNRc_test = {linear_to_db(SNRc_test.mean()):.2f} dB")
    print(f"Reward_test = {reward_test_mean:.4f}")


    # Ï†ÄÏû•
    result_dir = "data"
    os.makedirs(result_dir, exist_ok=True)

    torch.save({
        'losses': np.array(train_losses),
        'reward_ep': np.array(episode_reward_log),
        'snrt_ep': np.array(episode_snrt_log),
        'snrc_ep': np.array(episode_snrc_log),
        'snrt_test': linear_to_db(SNRt_test.mean()).cpu().numpy(),
        'snrc_test': linear_to_db(SNRc_test.mean()).cpu().numpy(),
        'reward_test': reward_test_mean,
    }, os.path.join(result_dir, f"td3_train.pt"))

    print(f"\n== Test Í≤∞Í≥ºÍ∞Ä '{result_dir}'Ïóê Ï†ÄÏû•ÎêòÏóàÏäµÎãàÎã§. ==")
