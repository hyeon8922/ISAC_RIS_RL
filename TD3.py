import torch
import numpy as np
from torch import nn, optim

############################################################
# 0) Ornstein-Uhlenbeck Noise Class (batch-aware)
############################################################
class OUNoise:
    """
    Ornstein–Uhlenbeck process (batch-aware).
    - mu, x0: scalar or array-like; broadcast to `shape`.
    - reset(shape=None, x0=None): call at episode start (optionally with shape).
    """
    def __init__(self, mu=0.0, theta=0.15, sigma=0.2, dt=1e-2, x0=None):
        self.mu = mu
        self.theta = float(theta)
        self.sigma = float(sigma)
        self.dt = float(dt)
        self.x0 = x0
        self.X = None  # internal state (np.ndarray)

    def _ensure_state(self, shape):
        """Create or resize internal state to match `shape`."""
        if (self.X is None) or (self.X.shape != tuple(shape)):
            if self.x0 is None:
                self.X = np.zeros(shape, dtype=np.float32)
            else:
                self.X = np.broadcast_to(self.x0, shape).astype(np.float32).copy()

    def reset(self, shape=None, x0=None):
        """Reset OU state."""
        if x0 is not None:
            self.x0 = x0
        if shape is None:
            # 다음 __call__에서 shape에 맞게 새로 잡도록 완전히 초기화
            self.X = None
        else:
            self._ensure_state(shape)

    def __call__(self, shape):
        """Generate OU noise with given `shape`."""
        self._ensure_state(shape)
        mu = np.broadcast_to(self.mu, shape).astype(np.float32)
        randn = np.random.randn(*shape).astype(np.float32)
        dx = self.theta * (mu - self.X) * self.dt + self.sigma * np.sqrt(self.dt) * randn
        self.X = self.X + dx
        return self.X


############################################################
# 1) ReplayBuffer  (state: 1D real, action: complex)
############################################################
class ReplayBuffer:
    def __init__(self, max_size=1e6):
        self.storage = []
        self.max_size = int(max_size)
        self.ptr = 0

    def add(self, data):
        """
        data: (state, action, reward, next_state, done)
        - state:      torch.Tensor [state_dim]
        - action:     torch.Tensor [N] (complex)
        - reward:     float
        - next_state: torch.Tensor [state_dim]
        - done:       float (0.0 or 1.0)
        """
        if len(self.storage) == self.max_size:
            self.storage[self.ptr % self.max_size] = data
        else:
            self.storage.append(data)
        self.ptr += 1

    def sample(self, batch_size):
        ind = np.random.randint(0, len(self.storage), size=batch_size)
        batch = [self.storage[i] for i in ind]
        states, actions, rewards, next_states, dones = zip(*batch)
        return (
            torch.stack(states),                                      # [B, state_dim]
            torch.stack(actions),                                     # [B, N] (complex)
            torch.tensor(rewards, dtype=torch.float32).unsqueeze(1),  # [B,1]
            torch.stack(next_states),                                 # [B, state_dim]
            torch.tensor(dones, dtype=torch.float32).unsqueeze(1)     # [B,1]
        )

    def __len__(self):
        return len(self.storage)


############################################################
# 2) Actor
############################################################
class Actor(nn.Module):
    def __init__(self, N, state_dim):
        super(Actor, self).__init__()
        self.N = N
        self.state_dim = state_dim

        self.fc1 = nn.Linear(state_dim, 128)
        self.fc2 = nn.Linear(128, 32)
        self.fc3 = nn.Linear(32, N)

    def forward(self, x):
        """
        x: [B, state_dim]
        return: theta, thetaH (각 [B, N], complex)
        """
        x = x.view(-1, self.state_dim)
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        phase = self.fc3(x)  # [B, N]

        theta_real = torch.cos(phase)
        theta_imag = torch.sin(phase)
        theta  = (theta_real + 1j * theta_imag).to(torch.complex64)
        thetaH = (theta_real - 1j * theta_imag).to(torch.complex64)
        return theta, thetaH

    def get_phase(self, x):
        """
        x: [B, state_dim]
        return: phase [B, N]
        """
        x = x.view(-1, self.state_dim)
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        phase = self.fc3(x)
        return phase


############################################################
# 3) Critic (Q-network)
############################################################
class Critic(nn.Module):
    def __init__(self, N, state_dim):
        super(Critic, self).__init__()
        self.N = N
        self.state_dim = state_dim

        # input: [state_dim] + [Re(action):N] + [Im(action):N]
        self.fc1 = nn.Linear(state_dim + 2 * N, 512)
        self.fc2 = nn.Linear(512, 256)
        self.fc3 = nn.Linear(256, 1)

    def forward(self, x, action):
        """
        x:      [B, state_dim]
        action: [B, N] (complex)
        """
        x = x.view(-1, self.state_dim)
        a_cat = torch.cat([torch.real(action), torch.imag(action)], dim=1)
        x = torch.cat([x, a_cat], dim=1)
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        q_value = self.fc3(x)
        return q_value


############################################################
# 4) TD3 Agent
############################################################
class TD3Agent:
    def __init__(self,
                 N,
                 device,
                 lr=1e-3,
                 gamma=0.99,
                 tau=0.005,
                 max_buffer_size=1e6,
                 use_ou_noise=True,
                 ou_rho=0.90,
                 ou_sigma=0.20,
                 policy_noise=0.2,      # target policy smoothing noise (phase 공간에서)
                 noise_clip=0.5,        # smoothing noise clip
                 policy_delay=2         # delayed policy update
                 ):
        self.N = N
        self.device = device
        self.gamma = gamma
        self.tau = tau
        self.policy_noise = policy_noise
        self.noise_clip = noise_clip
        self.policy_delay = policy_delay

        # state = flatten(4 x N x N) + 1 (prev SNR dB)
        self.state_dim = 4 * N * N + 2

        # Actor & Target
        self.actor = Actor(N, self.state_dim).to(device)
        self.actor_target = Actor(N, self.state_dim).to(device)
        self.actor_target.load_state_dict(self.actor.state_dict())
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr)

        # Two Critics & Targets
        self.critic1 = Critic(N, self.state_dim).to(device)
        self.critic2 = Critic(N, self.state_dim).to(device)
        self.critic1_target = Critic(N, self.state_dim).to(device)
        self.critic2_target = Critic(N, self.state_dim).to(device)
        self.critic1_target.load_state_dict(self.critic1.state_dict())
        self.critic2_target.load_state_dict(self.critic2.state_dict())
        self.critic1_optimizer = optim.Adam(self.critic1.parameters(), lr=lr)
        self.critic2_optimizer = optim.Adam(self.critic2.parameters(), lr=lr)

        # ReplayBuffer
        self.replay_buffer = ReplayBuffer(max_size=max_buffer_size)

        # OU Noise (exploration용, phase에 더함)
        self.use_ou_noise = use_ou_noise
        if self.use_ou_noise:
            theta_ou = float(-np.log(max(min(ou_rho, 0.9999), 1e-6)))  # theta = -log(rho)
            self.ou_noise = OUNoise(mu=0.0, theta=theta_ou,
                                    sigma=float(ou_sigma), dt=1.0, x0=None)
            self.ou_noise.reset()
        else:
            self.ou_noise = None

        # TD3 step counter
        self.total_it = 0

    def reset_noise(self):
        """에피소드 시작 시 OU 노이즈 상태 초기화."""
        if self.use_ou_noise and (self.ou_noise is not None):
            # shape=None → X=None, 이후 첫 호출에서 batch shape에 맞게 재생성
            self.ou_noise.reset(shape=None)

    @torch.no_grad()
    def select_action(self, state, noise=True):
        """
        state: [B, state_dim]
        return: theta, thetaH (복소수) [B, N]
        """
        state = state.to(self.device)
        phase = self.actor.get_phase(state)  # [B, N]

        # exploration noise (phase 공간)
        if noise and self.use_ou_noise and (self.ou_noise is not None):
            phase_np = phase.cpu().numpy()
            noise_val = self.ou_noise(phase_np.shape)
            phase = torch.from_numpy(phase_np + noise_val).to(self.device)

        theta_real = torch.cos(phase)
        theta_imag = torch.sin(phase)
        theta  = (theta_real + 1j * theta_imag).to(torch.complex64)
        thetaH = (theta_real - 1j * theta_imag).to(torch.complex64)
        return theta, thetaH

    def train(self, batch_size):
        if len(self.replay_buffer) < batch_size:
            return

        self.total_it += 1

        # replay에서 샘플링
        states, actions, rewards, next_states, dones = self.replay_buffer.sample(batch_size)
        states      = states.to(self.device)
        actions     = actions.to(self.device)
        rewards     = rewards.to(self.device)    # [B,1]
        next_states = next_states.to(self.device)
        dones       = dones.to(self.device)

        # ── 1) Target policy smoothing ─────────────────────────
        with torch.no_grad():
            # actor_target에서 next phase 얻기
            next_phase = self.actor_target.get_phase(next_states)  # [B, N]

            # TD3-style target noise (Gaussian) in phase space
            noise = torch.randn_like(next_phase) * self.policy_noise
            noise = torch.clamp(noise, -self.noise_clip, self.noise_clip)
            next_phase_noisy = next_phase + noise

            # phase → complex action
            next_theta_real = torch.cos(next_phase_noisy)
            next_theta_imag = torch.sin(next_phase_noisy)
            next_actions = (next_theta_real + 1j * next_theta_imag).to(torch.complex64)

            # 두 target critic 중 최소값 사용
            target_Q1 = self.critic1_target(next_states, next_actions)
            target_Q2 = self.critic2_target(next_states, next_actions)
            target_Q = torch.min(target_Q1, target_Q2)
            target_Q = rewards + (1 - dones) * self.gamma * target_Q

        # ── 2) Critic1 update ──────────────────────────────────
        current_Q1 = self.critic1(states, actions)
        critic1_loss = nn.MSELoss()(current_Q1, target_Q)
        self.critic1_optimizer.zero_grad()
        critic1_loss.backward()
        self.critic1_optimizer.step()

        # ── 3) Critic2 update ──────────────────────────────────
        current_Q2 = self.critic2(states, actions)
        critic2_loss = nn.MSELoss()(current_Q2, target_Q)
        self.critic2_optimizer.zero_grad()
        critic2_loss.backward()
        self.critic2_optimizer.step()

        # ── 4) Delayed Actor & Target update ───────────────────
        if self.total_it % self.policy_delay == 0:
            # Actor: maximize Q1 → minimize -Q1
            phase = self.actor.get_phase(states)
            theta_real = torch.cos(phase)
            theta_imag = torch.sin(phase)
            current_actions = (theta_real + 1j * theta_imag).to(torch.complex64)

            actor_loss = -self.critic1(states, current_actions).mean()
            self.actor_optimizer.zero_grad()
            actor_loss.backward()
            self.actor_optimizer.step()

            # Soft update targets
            with torch.no_grad():
                for param, target_param in zip(self.critic1.parameters(),
                                               self.critic1_target.parameters()):
                    target_param.data.copy_(
                        self.tau * param.data + (1 - self.tau) * target_param.data
                    )
                for param, target_param in zip(self.critic2.parameters(),
                                               self.critic2_target.parameters()):
                    target_param.data.copy_(
                        self.tau * param.data + (1 - self.tau) * target_param.data
                    )
                for param, target_param in zip(self.actor.parameters(),
                                               self.actor_target.parameters()):
                    target_param.data.copy_(
                        self.tau * param.data + (1 - self.tau) * target_param.data
                    )
